{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    accuracy_score, \n",
    "    roc_curve, \n",
    "    roc_auc_score\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset\n",
    "df = pd.read_csv(\"inputs-for-ml/final_ml_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "feature_cols = ['elevation', 'slope', 'north_gps', 'east_gps', 'vertical_gps', 'coherence', 'los_insar', 'bias']\n",
    "X = df[feature_cols]\n",
    "y = df['needs_calibration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train/test split (Taking 70-30)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.3, random_state=42, stratify=y\n",
    ") #42 from hhgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to train and evaluate models\n",
    "def train_and_evaluate(model):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    name = model.__class__.__name__\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"\\n{name}\")\n",
    "    print(cm)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Accuracy:\", acc)\n",
    "\n",
    "    y_prob = None\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    elif hasattr(model, \"decision_function\"):\n",
    "        decision_scores = model.decision_function(X_test)\n",
    "        y_prob = MinMaxScaler().fit_transform(decision_scores.reshape(-1, 1)).ravel()\n",
    "\n",
    "    if y_prob is not None:\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "        auc_score = roc_auc_score(y_test, y_prob)\n",
    "    else:\n",
    "        fpr, tpr, auc_score = None, None, np.nan\n",
    "\n",
    "    return {\n",
    "        'Model': name,\n",
    "        'Accuracy': acc,\n",
    "        'ROC AUC': auc_score,\n",
    "        'Confusion Matrix': cm,\n",
    "        'ROC Curve': (fpr, tpr, auc_score) if fpr is not None else None\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of models to compare\n",
    "models = [\n",
    "    SVC(probability=True),\n",
    "    MLPClassifier(max_iter=1000),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    GaussianNB(),\n",
    "    GaussianProcessClassifier(kernel=1.0 * RBF(length_scale=1.0))\n",
    "]\n",
    "\n",
    "# Evaluate all models and collect results\n",
    "results = [train_and_evaluate(model) for model in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate results into performance metrics and plots\n",
    "performance_records = []\n",
    "confusion_matrices = {}\n",
    "roc_curves = {}\n",
    "\n",
    "for res in results:\n",
    "    performance_records.append({\n",
    "        'Model': res['Model'],\n",
    "        'Accuracy': res['Accuracy'],\n",
    "        'ROC AUC': res['ROC AUC']\n",
    "    })\n",
    "    confusion_matrices[res['Model']] = res['Confusion Matrix']\n",
    "    if res['ROC Curve']:\n",
    "        roc_curves[res['Model']] = res['ROC Curve']\n",
    "\n",
    "# Create DataFrame to display model performance\n",
    "performance_df = pd.DataFrame(performance_records)\n",
    "performance_df = performance_df.sort_values(by=['Accuracy', 'ROC AUC'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "for name, (fpr, tpr, auc_score) in roc_curves.items():\n",
    "    plt.plot(fpr, tpr, label=f\"{name}\")\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot individual confusion matrices\n",
    "for name, cm in confusion_matrices.items():\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=[0, 1], yticklabels=[0, 1], ax=ax, cbar=False)\n",
    "    ax.set_title(f'Confusion Matrix - {name}', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Predicted Label')\n",
    "    ax.set_ylabel('True Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the performance table\n",
    "print(\"\\nModel Performance Ranking:\")\n",
    "print(performance_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
