{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    accuracy_score, \n",
    "    roc_curve, \n",
    "    roc_auc_score\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset\n",
    "df = pd.read_csv(\"inputs-for-ml/final_ml_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "feature_cols = ['elevation', 'slope', 'north_gps', 'east_gps', 'vertical_gps', 'coherence', 'los_insar', 'bias']\n",
    "X = df[feature_cols]\n",
    "y = df['needs_calibration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train/test split (Taking 70-30)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.3, random_state=42, stratify=y\n",
    ") #42 from hhgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to train and evaluate models\n",
    "def train_and_evaluate(model):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    name = model.__class__.__name__\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"\\n{name}\")\n",
    "    print(cm)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Accuracy:\", acc)\n",
    "\n",
    "    y_prob = None\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    elif hasattr(model, \"decision_function\"):\n",
    "        decision_scores = model.decision_function(X_test)\n",
    "        y_prob = MinMaxScaler().fit_transform(decision_scores.reshape(-1, 1)).ravel()\n",
    "\n",
    "    if y_prob is not None:\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "        auc_score = roc_auc_score(y_test, y_prob)\n",
    "    else:\n",
    "        fpr, tpr, auc_score = None, None, np.nan\n",
    "\n",
    "    return {\n",
    "        'Model': name,\n",
    "        'Accuracy': acc,\n",
    "        'ROC AUC': auc_score,\n",
    "        'Confusion Matrix': cm,\n",
    "        'ROC Curve': (fpr, tpr, auc_score) if fpr is not None else None\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of models to compare\n",
    "models = [\n",
    "    SVC(probability=True),\n",
    "    MLPClassifier(max_iter=1000),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    GaussianNB(),\n",
    "    GaussianProcessClassifier(kernel=1.0 * RBF(length_scale=1.0))\n",
    "]\n",
    "\n",
    "# Evaluate all models and collect results\n",
    "results = [train_and_evaluate(model) for model in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate results into performance metrics and plots\n",
    "performance_records = []\n",
    "confusion_matrices = {}\n",
    "roc_curves = {}\n",
    "\n",
    "for res in results:\n",
    "    performance_records.append({\n",
    "        'Model': res['Model'],\n",
    "        'Accuracy': res['Accuracy'],\n",
    "        'ROC AUC': res['ROC AUC']\n",
    "    })\n",
    "    confusion_matrices[res['Model']] = res['Confusion Matrix']\n",
    "    if res['ROC Curve']:\n",
    "        roc_curves[res['Model']] = res['ROC Curve']\n",
    "\n",
    "# Create DataFrame to display model performance\n",
    "performance_df = pd.DataFrame(performance_records)\n",
    "performance_df = performance_df.sort_values(by=['Accuracy', 'ROC AUC'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "for name, (fpr, tpr, auc_score) in roc_curves.items():\n",
    "    plt.plot(fpr, tpr, label=f\"{name}\")\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot individual confusion matrices\n",
    "for name, cm in confusion_matrices.items():\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=[0, 1], yticklabels=[0, 1], ax=ax, cbar=False)\n",
    "    ax.set_title(f'Confusion Matrix - {name}', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Predicted Label')\n",
    "    ax.set_ylabel('True Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the performance table\n",
    "print(\"\\nModel Performance Ranking:\")\n",
    "print(performance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended Hyperparameter Tuning for Classification Models\n",
    "\n",
    "# Define classification models with hyperparameter grids\n",
    "param_grids_classification = {\n",
    "    \"Support Vector Classifier\": {\n",
    "        \"model\": SVC(probability=True, random_state=42),\n",
    "        \"params\": {\n",
    "            \"C\": [0.1, 1, 10],\n",
    "            \"kernel\": [\"rbf\", \"linear\"]\n",
    "        }\n",
    "    },\n",
    "    \"Neural Network Classifier\": {\n",
    "        \"model\": MLPClassifier(max_iter=1000, random_state=42),\n",
    "        \"params\": {\n",
    "            \"hidden_layer_sizes\": [(50,), (100,), (100, 50)],\n",
    "            \"alpha\": [0.0001, 0.001],\n",
    "            \"learning_rate_init\": [0.001, 0.01]\n",
    "        }\n",
    "    },\n",
    "    \"Decision Tree Classifier\": {\n",
    "        \"model\": DecisionTreeClassifier(random_state=42),\n",
    "        \"params\": {\n",
    "            \"max_depth\": [None, 10, 20],\n",
    "            \"min_samples_split\": [2, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    \"Random Forest Classifier\": {\n",
    "        \"model\": RandomForestClassifier(random_state=42),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [50, 100, 200],\n",
    "            \"max_depth\": [None, 10, 20],\n",
    "            \"min_samples_split\": [2, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    \"Gradient Boosting Classifier\": {\n",
    "        \"model\": GradientBoostingClassifier(random_state=42),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": [50, 100, 200],\n",
    "            \"learning_rate\": [0.01, 0.1],\n",
    "            \"max_depth\": [3, 5, 7]\n",
    "        }\n",
    "    },\n",
    "    \"Naive Bayes Classifier\": {\n",
    "        \"model\": GaussianNB(),\n",
    "        \"tune\": False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize dictionaries to store best models and feature importances\n",
    "best_classifiers = {}\n",
    "class_feature_importance = {}\n",
    "\n",
    "# Train and tune classifiers\n",
    "for name, settings in param_grids_classification.items():\n",
    "    model = settings[\"model\"]\n",
    "    if settings.get(\"tune\", True):\n",
    "        grid_search = GridSearchCV(model, settings[\"params\"], cv=5, scoring=\"accuracy\", n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_model = grid_search.best_estimator_\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        best_model = model\n",
    "\n",
    "    best_classifiers[name] = best_model\n",
    "\n",
    "    # Extract feature importances\n",
    "    if hasattr(best_model, \"feature_importances_\"):\n",
    "        class_feature_importance[name] = best_model.feature_importances_\n",
    "    elif hasattr(best_model, \"coef_\"):\n",
    "        class_feature_importance[name] = np.abs(best_model.coef_).flatten()\n",
    "    else:\n",
    "        result = permutation_importance(best_model, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "        class_feature_importance[name] = result.importances_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create DataFrame for feature importance\n",
    "feature_importance_df_class = pd.DataFrame(class_feature_importance, index=feature_cols)\n",
    "feature_importance_df_class = feature_importance_df_class.clip(lower=0)\n",
    "\n",
    "# Normalize feature importance values across models\n",
    "feature_importance_df_class = feature_importance_df_class.div(feature_importance_df_class.sum(axis=0), axis=1)\n",
    "\n",
    "# Plot feature importances across models\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "feature_importance_df_class.plot(kind=\"barh\", ax=ax, colormap=\"tab10\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"Feature Importance Comparison Across Classification Models\", fontsize=14, weight=\"bold\")\n",
    "plt.xlabel(\"Normalized Importance Score\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print feature importance table\n",
    "print(\"\\nNormalized Feature Importance (Classification Models):\")\n",
    "print(feature_importance_df_class.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all tuned classifiers\n",
    "performance_list_class = []\n",
    "\n",
    "for name, model in best_classifiers.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    elif hasattr(model, \"decision_function\"):\n",
    "        decision_scores = model.decision_function(X_test)\n",
    "        y_prob = MinMaxScaler().fit_transform(decision_scores.reshape(-1, 1)).ravel()\n",
    "    else:\n",
    "        y_prob = np.zeros_like(y_pred)  # fallback if no probability\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_prob)\n",
    "    \n",
    "    performance_list_class.append({\"Model\": name, \"Accuracy\": acc, \"ROC AUC\": roc_auc})\n",
    "\n",
    "# Create performance DataFrame\n",
    "performance_df_class = pd.DataFrame(performance_list_class)\n",
    "performance_df_class.rename(columns=lambda x: x.strip().upper(), inplace=True)\n",
    "performance_df_class = performance_df_class.round({\"ACCURACY\": 3, \"ROC AUC\": 3})\n",
    "performance_df_class = performance_df_class.sort_values(by=\"ROC AUC\", ascending=False).reset_index(drop=True)\n",
    "performance_df_class.index += 1\n",
    "performance_df_class.index.name = \"Rank\"\n",
    "\n",
    "# Print classification performance\n",
    "print(\"\\nTuned Classification Model Performance:\")\n",
    "print(performance_df_class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
